{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:09:23.978660Z",
     "start_time": "2025-07-27T14:09:21.671453Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer"
   ],
   "id": "4a18e181a3f664cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahas/PycharmProjects/STIRS/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mahas/PycharmProjects/STIRS/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up Parameters and Loading Data"
   ],
   "id": "8e5fcfcef8390723"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:09.550010Z",
     "start_time": "2025-07-27T14:12:04.807691Z"
    }
   },
   "source": [
    "# Setting up the parameters\n",
    "maximum_features = 30522  # Maximum number of words to consider as features\n",
    "maximum_length = 128  # Maximum length of input sequences\n",
    "word_embedding_dims = 50  # Dimension of word embeddings\n",
    "no_of_filters = 128  # Number of filters in the convolutional layer\n",
    "kernel_size = 3  # Size of the convolutional filters\n",
    "hidden_dim_1 = 128  # Number of neurons in the hidden layer\n",
    "\n",
    "batch_size = 64  # Batch size for training\n",
    "epochs = 10  # Number of training epochs\n",
    "threshold = 0.7  # Threshold for binary classification\n",
    "\n",
    "DATASET_SIZE = 10_000\n",
    "\n",
    "df = pd.read_csv(\"../jigsaw/dataset_text_target.csv\")\n",
    "df_true = df[df.target > threshold]\n",
    "df_false = df[df.target <= threshold]\n",
    "df = pd.concat([df_true[DATASET_SIZE // 2:DATASET_SIZE], df_false[DATASET_SIZE // 2:DATASET_SIZE]], axis=0)\n",
    "mapper = lambda x: 1 if x > 0.5 else 0\n",
    "df.target = df.target.apply(mapper)\n"
   ],
   "id": "f69a4d0be88a5af2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:09.600214Z",
     "start_time": "2025-07-27T14:12:09.597880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "x_test = df.comment_text\n",
    "y_test = df.target"
   ],
   "id": "2580b1423b1f1356",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:17.626037Z",
     "start_time": "2025-07-27T14:12:09.604961Z"
    }
   },
   "source": [
    "# Tokenize and encode the data using the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "X_test_encoded = tokenizer.batch_encode_plus(\n",
    "    x_test.tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=maximum_length,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "test_dataset = TensorDataset(X_test_encoded['input_ids'], torch.tensor(y_test.values, dtype=torch.float32))\n",
    "\n",
    "# Create DataLoaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "b37684832e0b93a9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:17.735267Z",
     "start_time": "2025-07-27T14:12:17.696130Z"
    }
   },
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_size, hidden_dim):\n",
    "        super(CNNTextClassifier, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, n_filters, kernel_size=filter_size, padding='valid')\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_filters, n_filters, kernel_size=filter_size, padding='valid')\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(n_filters, n_filters, kernel_size=filter_size, padding='valid')\n",
    "        # Global Max Pooling is achieved with AdaptiveMaxPool1d\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(n_filters, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids shape: (batch_size, seq_len)\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # PyTorch Conv1d expects (batch_size, channels, seq_len)\n",
    "        # So we permute the dimensions\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "\n",
    "        x = self.pool1(F.relu(self.conv1(embedded)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = self.global_pool(x).squeeze(2)  # Squeeze to remove the last dimension\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNTextClassifier(\n",
    "    vocab_size=maximum_features,\n",
    "    embedding_dim=word_embedding_dims,\n",
    "    n_filters=no_of_filters,\n",
    "    filter_size=kernel_size,\n",
    "    hidden_dim=hidden_dim_1\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"../cnn_model_trained_torch/cnn_model_trained_pytorch.pth\"))\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "model.to(device)"
   ],
   "id": "39231c0639677dba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNTextClassifier(\n",
       "  (embedding): Embedding(30522, 50)\n",
       "  (conv1): Conv1d(50, 128, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "  (global_pool): AdaptiveMaxPool1d(output_size=1)\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the Model\n",
    "\n",
    "We define the loss function and optimizer, then write an explicit loop to train the model over 10 epochs and validate its performance."
   ],
   "id": "17796eab2150523e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:18.310057Z",
     "start_time": "2025-07-27T14:12:17.766913Z"
    }
   },
   "source": [
    "model.eval()\n",
    "y_pred_prob = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, labels in test_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        outputs = model(input_ids).squeeze()\n",
    "        y_pred_prob.extend(outputs.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "y_pred = (np.array(y_pred_prob) > threshold).astype(int)\n",
    "\n",
    "# Calculating and printing evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_true, y_pred))"
   ],
   "id": "9a862e945befca7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.97      0.94      4843\n",
      "         1.0       0.97      0.91      0.94      5157\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:18.346280Z",
     "start_time": "2025-07-27T14:12:18.342984Z"
    }
   },
   "cell_type": "code",
   "source": "y_pred",
   "id": "9f09a707733ebe5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:18.384440Z",
     "start_time": "2025-07-27T14:12:18.379948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = x_test.tolist()\n",
    "predicted = y_pred.tolist()\n",
    "truth = y_test.tolist()\n",
    "size = len(sentences)\n",
    "\n",
    "mismatches = []\n",
    "for i in range(size):\n",
    "    if predicted[i] == truth[i]:\n",
    "        continue\n",
    "    mismatches.append((sentences[i], predicted[i], truth[i]))"
   ],
   "id": "2ca5207b483b7b2a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:18.421437Z",
     "start_time": "2025-07-27T14:12:18.414547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "errors = pd.DataFrame(mismatches, columns=['sentence', 'predicted', 'truth'])\n",
    "errors.to_csv(\"cnn_errors_pt.csv\", index=False)"
   ],
   "id": "ccd505cc4373c1b8",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
